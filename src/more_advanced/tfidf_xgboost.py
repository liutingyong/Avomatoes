#we are aiming for higher accuracy
#vectorizing first --> turning into a numerical list, numerical fingerprint like

#tf-idf:
#tf = term frequency, number of times a word appears in a doc / total terms in a doc
#idf = inverse document frequency, how many docs contain the word (x) vs total docs (n) -> log(n/(1+x))
#tf-idf score is the product of the two, rare words have greater weights



